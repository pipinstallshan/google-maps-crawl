{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "import zipfile\n",
    "import asyncio\n",
    "import openpyxl\n",
    "import requests\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from requests_html import HTMLSession \n",
    "from requests_html import AsyncHTMLSession \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "# from selenium_driverless import webdriver\n",
    "# from selenium_driverless.types.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from websockets.exceptions import ConnectionClosedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeIds = []\n",
    "placeDetails = []\n",
    "\n",
    "api_key = \"AIzaSyBtlL0t_zC_m6Ad6BQ02udeH1B77B5LWno\"\n",
    "workbook = openpyxl.load_workbook('./doc/Input 2nd 10k.xlsx')\n",
    "sheet = workbook.active\n",
    "\n",
    "column_A = [cell.value for cell in sheet['A'][1:]]\n",
    "_id = [entry.split(';', 1)[0].replace(\";\", \" \").replace('\"','').strip() for entry in column_A]\n",
    "names = [entry.split(';', 1)[1].replace(\";\", \" \").replace('\"','').replace(\"  \", \" \") for entry in column_A]\n",
    "print(_id, \"\\n\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "placeIds = []\n",
    "alreadyCrawledIds = []\n",
    "\n",
    "# with open('placeIds_v2.json', 'r', encoding='utf-8') as f:\n",
    "#     placeReadIds = json.load(f)\n",
    "#     for item in placeReadIds:\n",
    "#         __id = item['_id']\n",
    "#         alreadyCrawledIds.append(__id)\n",
    "\n",
    "for iter_id, __name in zip(_id, names):\n",
    "    item = {}\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "    if iter_id not in alreadyCrawledIds:\n",
    "        # try:\n",
    "            print(\"\\n\", __name)\n",
    "            getPlaceId = f\"https://maps.googleapis.com/maps/api/place/textsearch/json?query={__name}&key={api_key}\"\n",
    "            response = requests.get(getPlaceId)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                print(data)\n",
    "#                 if data['status'] != \"ZERO_RESULTS\":\n",
    "#                     pId = data['results'][0]['place_id']\n",
    "#                     item = {\"_id\": iter_id, \"name\": __name, \"placeId\": pId}\n",
    "#                     with open('tempPlaceIds_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                         json.dump(item, f, ensure_ascii=False, indent=2)\n",
    "#                         f.write(\",\\n\")\n",
    "#                     placeIds.append(item)\n",
    "#                     print(item)\n",
    "#                 else:\n",
    "#                     print(f\"Error: Location not found\")\n",
    "#                     item = {\"_id\": iter_id, \"name\": __name, \"placeId\": \"\"}\n",
    "#                     with open('tempPlaceIds_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                         json.dump(item, f, ensure_ascii=False, indent=2)\n",
    "#                         f.write(\",\\n\")\n",
    "#                     placeIds.append(item)\n",
    "#             else:\n",
    "#                 print(f\"Error: {response.status_code}\")\n",
    "#                 item = {\"_id\": iter_id, \"name\": __name, \"placeId\": \"\"}\n",
    "#                 with open('tempPlaceIds_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                     json.dump(item, f, ensure_ascii=False, indent=2)\n",
    "#                     f.write(\",\\n\")\n",
    "#                 placeIds.append(item)\n",
    "#         except Exception as e:\n",
    "#             print(\"Exception PlaceIds: \", e)\n",
    "#             item = {\"_id\": iter_id, \"name\": __name, \"placeId\": \"\"}\n",
    "#             with open('tempPlaceIds_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                 json.dump(item, f, ensure_ascii=False, indent=2)\n",
    "#                 f.write(\",\\n\")\n",
    "#             placeIds.append(item)\n",
    "#     else:\n",
    "#         print(\"ALREADY_CRAWLED: \", iter_id)\n",
    "        \n",
    "# print(placeIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeIds = []\n",
    "with open('placeIds_v2.json', 'r', encoding='utf-8') as f:\n",
    "    placeReadIds = json.load(f)\n",
    "    for item in placeReadIds:\n",
    "        placeIds.append(item)\n",
    "print(placeIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# placeDetails = []\n",
    "# for place in placeIds:\n",
    "#     try:\n",
    "#         getPlaceDetails = f\"https://maps.googleapis.com/maps/api/place/details/json?place_id={place['placeId']}&fields=url,user_ratings_total,name,rating,formatted_phone_number,formatted_address,international_phone_number,place_id&key={api_key}\"\n",
    "#         response = requests.get(getPlaceDetails)\n",
    "#         if response.status_code == 200:\n",
    "#             data = response.json()\n",
    "#             counter += 1\n",
    "#             print(counter)\n",
    "#             if data['status'] != \"INVALID_REQUEST\":\n",
    "#                 data.pop('html_attributions', '')\n",
    "#                 data['_id'] = place['_id']\n",
    "#                 with open('placeDetails_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                     json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "#                     f.write(\",\\n\")\n",
    "#                 placeDetails.append(data)\n",
    "#             else:\n",
    "#                 print(f\"Error: PlaceDetails not found\")\n",
    "#                 data['_id'] = place['_id']\n",
    "#                 data['result'] = {}\n",
    "#                 data['result']['url'] = \"\"\n",
    "#                 data['result']['name'] = \"\"\n",
    "#                 with open('placeDetails_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                     json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "#                     f.write(\",\\n\")\n",
    "#                 placeDetails.append(data)\n",
    "#         else:\n",
    "#             print(f\"Error: PlaceDetails not found\")\n",
    "#             data['_id'] = place['_id']\n",
    "#             data['result'] = {}\n",
    "#             data['result']['url'] = \"\"\n",
    "#             data['result']['name'] = \"\"\n",
    "#             with open('placeDetails_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                 json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "#                 f.write(\",\\n\")\n",
    "#             placeDetails.append(data)\n",
    "#     except:\n",
    "#         data = []\n",
    "#         print(f\"Error: PlaceDetails not found\")\n",
    "#         data['_id'] = place['_id']\n",
    "#         data['result'] = {}\n",
    "#         data['result']['url'] = \"\"\n",
    "#         data['result']['name'] = \"\"\n",
    "#         with open('placeDetails_v2.json', 'a', encoding='utf-8') as f:\n",
    "#             json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "#             f.write(\",\\n\")\n",
    "#         placeDetails.append(data)\n",
    "\n",
    "# print(placeDetails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('placeIds_v2.json', 'r', encoding='utf-8') as f:\n",
    "    placeIds = json.load(f)\n",
    "print(len(placeIds))\n",
    "\n",
    "with open('placeDetails_v2.json', 'r', encoding='utf-8') as f:\n",
    "    placeDetails = json.load(f)\n",
    "print(len(placeDetails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have used selenium_driverless:\n",
    "\n",
    "# googleSerp = []\n",
    "# googleSerpIds = []\n",
    "\n",
    "# try:\n",
    "#     with open('googleSerp_v2.json', 'r', encoding='utf-8') as f:\n",
    "#         googleSerp = json.load(f)\n",
    "#         if googleSerp:\n",
    "#             for item in googleSerp:\n",
    "#                 for key, value in item.items():\n",
    "#                     googleSerpIds.append(key)\n",
    "\n",
    "#     print(len(googleSerpIds))\n",
    "# except:\n",
    "#     print(\"NO PREVIOUS RECORD FOUND!\")\n",
    "#     pass\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# async def get_company_info(url):\n",
    "#     proxies = {\n",
    "#         'http': 'http://waleeProxy:Cheeta@1@au.proxymesh.com:31280',\n",
    "#         'https': 'http://waleeProxy:Cheeta@1@au.proxymesh.com:31280',\n",
    "#     }\n",
    "#     ua = UserAgent()\n",
    "#     headers = {\n",
    "#         \"User-Agent\": f\"{ua.random}\"\n",
    "#     }\n",
    "#     session = AsyncHTMLSession()\n",
    "\n",
    "#     try:\n",
    "#         response = await session.get(url, headers=headers, proxies=proxies)\n",
    "#         await response.html.arender()\n",
    "#         html_content = response.html.html\n",
    "#         soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#         main_div = soup.find('div', {'role': 'main'})\n",
    "\n",
    "#         if main_div:\n",
    "#             company_url_element = main_div.find('a', {'data-tooltip': 'Open website'})\n",
    "#             company_url_temp = company_url_element.get('href') if company_url_element else None\n",
    "#             service_element = main_div.find('div', {'class': 'lMbq3e'}).find('div', {'class': 'fontBodyMedium'})\n",
    "#             following_sibling = service_element.find_next_sibling('div')\n",
    "#             service_var = following_sibling.text if following_sibling else \"\"\n",
    "            \n",
    "#             company_url = company_url_temp.split(\"q=\")[1].split(\"&opi\")[0] if company_url_temp else None\n",
    "            \n",
    "#             await session.close()\n",
    "#             return company_url, service_var\n",
    "#         return \"\", \"\"\n",
    "#     except Exception as e:\n",
    "#         await session.close()\n",
    "#         return \"\", \"\"\n",
    "        \n",
    "# async def main():\n",
    "#         options = webdriver.ChromeOptions()\n",
    "#         user_agent = \"Mozilla/5.0 (Linux; Android 9; SM-N975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.101 Mobile Safari/537.36\"\n",
    "#         options.add_argument(f'--user-agent={user_agent}')\n",
    "#         printCounter = 0\n",
    "        \n",
    "#         async with webdriver.Chrome(options=options) as driver:\n",
    "#             await driver.maximize_window()\n",
    "            \n",
    "#             counterPlace = 0\n",
    "#             for place in placeDetails:\n",
    "#                 try:\n",
    "#                     print(\"\\n\", printCounter, \":\")\n",
    "#                     printCounter += 1\n",
    "#                     if place['_id'] not in googleSerpIds:\n",
    "#                         url = place['result']['url']\n",
    "#                         if url != '':\n",
    "#                             await driver.get(url, wait_load=True, timeout=150)                            \n",
    "#                             time.sleep(5)\n",
    "                            \n",
    "#                             service_var = \"\"\n",
    "#                             company_url = \"\"\n",
    "#                             try:\n",
    "#                                 company_url, service_var = await get_company_info(url)\n",
    "#                             except:\n",
    "#                                 company_url, service_var = \"\", \"\"\n",
    "#                                 pass\n",
    "                            \n",
    "#                             try:\n",
    "#                                 await WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"lMbq3e\"]//span[contains(text(), \"No review\")]')))\n",
    "#                                 no_reviews = await driver.find_element(By.XPATH, '//div[@class=\"lMbq3e\"]//span[contains(text(), \"No review\")]')\n",
    "#                                 if no_reviews:\n",
    "#                                     placeDetails[counterPlace]['result']['reviews'] = []\n",
    "#                                     print(placeDetails[counterPlace]['result']['name'])\n",
    "                                    \n",
    "#                                     item = {}\n",
    "#                                     item[place['_id']] = {\n",
    "#                                         'company_info': True,\n",
    "#                                         'company_name': place['result']['name'] if 'name' in place['result'] else \"\",\n",
    "#                                         'address': place['result']['formatted_address'] if 'formatted_address' in place['result'] else \"\",\n",
    "#                                         'phone_number': place['result']['formatted_phone_number'] if 'formatted_phone_number' in place['result'] else \"\",\n",
    "#                                         'international_phone_number': place['result']['international_phone_number'] if 'international_phone_number' in place['result'] else \"\",\n",
    "#                                         'avg_customer_reviews': place['result']['rating'] if 'rating' in place['result'] else \"\",\n",
    "#                                         'no_of_customer_reviews': place['result']['user_ratings_total'] if 'user_ratings_total' in place['result'] else \"\",\n",
    "#                                         'service': service_var,\n",
    "#                                         'reviews': place['result']['reviews'] if 'reviews' in place['result'] else [],\n",
    "#                                         'google_maps_url': place['result']['url'] if 'url' in place['result'] else \"\",\n",
    "#                                         'company_url': company_url\n",
    "#                                     }\n",
    "                                    \n",
    "#                                     with open('tempGoogleSerp_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                                         json.dump(item, f, ensure_ascii=False, indent=2)\n",
    "#                                         f.write(',\\n')\n",
    "            \n",
    "#                                     counterPlace += 1\n",
    "#                                     continue\n",
    "#                             except:\n",
    "#                                 pass\n",
    "                            \n",
    "#                             await WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"lMbq3e\"]//span[contains(text(), \"review\")]')))\n",
    "#                             reviews_click = await driver.find_element(By.XPATH, '//div[@class=\"lMbq3e\"]//span[contains(text(), \"review\")]')\n",
    "#                             await reviews_click.click()       \n",
    "#                             time.sleep(3)\n",
    "                            \n",
    "#                             counter = 0\n",
    "#                             break_bool = False\n",
    "#                             scroll_counter = 0\n",
    "#                             prev_scroll_height = 0\n",
    "#                             while True:\n",
    "#                                 all_reviews = await driver.find_elements(By.XPATH, '//div[@class=\"hjmQqc\"]//div[@class=\"QGH3wd Inlyae\"]')\n",
    "#                                 if len(all_reviews) > counter:\n",
    "#                                     counter += 1\n",
    "                                    \n",
    "#                                 total_scroll_counter = 0 \n",
    "#                                 for review in all_reviews[counter-1:counter]:\n",
    "#                                     if total_scroll_counter <= 120:\n",
    "#                                         total_scroll_counter += 1\n",
    "#                                         await WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, '//div[@class=\"hjmQqc\"]//div[@class=\"QGH3wd Inlyae\"]')))\n",
    "#                                         await driver.execute_script(\"arguments[0].scrollIntoView(true);\", review)        \n",
    "#                                         time.sleep(0.3)\n",
    "#                                         current_scroll_height = await driver.execute_script(\"return Math.max( document.evaluate(\\\"//div[@class='nkePVe']\\\", document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue.scrollHeight );\")\n",
    "#                                         if current_scroll_height == prev_scroll_height:\n",
    "#                                             scroll_counter += 1\n",
    "#                                         else:\n",
    "#                                             scroll_counter = 0\n",
    "                                            \n",
    "#                                         if scroll_counter > 9 and current_scroll_height == prev_scroll_height:\n",
    "#                                             break_bool = True\n",
    "#                                             break\n",
    "\n",
    "#                                         prev_scroll_height = current_scroll_height\n",
    "#                                     else:\n",
    "#                                         break_bool = True\n",
    "#                                         break\n",
    "#                                 if break_bool:\n",
    "#                                     break\n",
    "                            \n",
    "#                             visible_reviews = await driver.find_elements(By.XPATH, '//div[@class=\"hjmQqc\"]')\n",
    "#                             reviews = []\n",
    "#                             if visible_reviews:\n",
    "#                                 for index, review in enumerate(visible_reviews):\n",
    "#                                     item = {}\n",
    "#                                     html_content = await review.get_attribute('outerHTML')\n",
    "#                                     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#                                     item['Counter'] = index + 1\n",
    "#                                     name_element = soup.find('div', class_='IaK8zc CVo7Bb')\n",
    "#                                     name = name_element.text\n",
    "#                                     item['Customer'] = name\n",
    "#                                     rating_element = soup.find('div', class_='nSAWhf').find('div')\n",
    "#                                     rating = rating_element['aria-label'].split(\"Rating of \")[1]\n",
    "#                                     item['Stars'] = rating\n",
    "#                                     time_element = soup.find('div', class_='bHyEBc')\n",
    "#                                     time_t = time_element.text\n",
    "#                                     item['Time'] = time_t\n",
    "#                                     description_element = soup.find('div', class_='nSAWhf').find('span')\n",
    "#                                     description = description_element.text\n",
    "#                                     item['Review'] = description\n",
    "#                                     reviews.append(item)\n",
    "#                                     if index >= 49:\n",
    "#                                         break\n",
    "#                             else:\n",
    "#                                 pass\n",
    "\n",
    "#                             placeDetails[counterPlace]['result']['reviews'] = reviews\n",
    "#                             print(placeDetails[counterPlace]['result']['name'])\n",
    "                            \n",
    "#                             item = {}\n",
    "#                             item[place['_id']] = {\n",
    "#                                 'company_info': True,\n",
    "#                                 'company_name': place['result']['name'] if 'name' in place['result'] else \"\",\n",
    "#                                 'address': place['result']['formatted_address'] if 'formatted_address' in place['result'] else \"\",\n",
    "#                                 'phone_number': place['result']['formatted_phone_number'] if 'formatted_phone_number' in place['result'] else \"\",\n",
    "#                                 'international_phone_number': place['result']['international_phone_number'] if 'international_phone_number' in place['result'] else \"\",\n",
    "#                                 'avg_customer_reviews': place['result']['rating'] if 'rating' in place['result'] else \"\",\n",
    "#                                 'no_of_customer_reviews': place['result']['user_ratings_total'] if 'user_ratings_total' in place['result'] else \"\",\n",
    "#                                 'service': service_var,\n",
    "#                                 'reviews': place['result']['reviews'] if 'reviews' in place['result'] else [],\n",
    "#                                 'google_maps_url': place['result']['url'] if 'url' in place['result'] else \"\",\n",
    "#                                 'company_url': company_url\n",
    "#                             }\n",
    "#                             with open('tempGoogleSerp_v2.json', 'a', encoding='utf-8') as f:\n",
    "#                                 json.dump(item, f, ensure_ascii=False, indent=2)\n",
    "#                                 f.write(',\\n')\n",
    "#                             counterPlace += 1                          \n",
    "#                         else:\n",
    "#                             counterPlace += 1\n",
    "#                             continue\n",
    "#                     else:\n",
    "#                         print(\"ALREADY_CRAWLED: \", place['result']['name'])\n",
    "#                         counterPlace += 1\n",
    "#                         continue\n",
    "#                 except:\n",
    "#                     print(\"ERROR PARSING: \", place['result']['name'])\n",
    "#                     counterPlace += 1\n",
    "#                     continue\n",
    "            \n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlugin(proxy_host, proxy_port, proxy_user, proxy_pass):\n",
    "    manifest_json = \"\"\"\n",
    "    {\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"manifest_version\": 2,\n",
    "        \"name\": \"Chrome Proxy\",\n",
    "        \"permissions\": [\n",
    "            \"proxy\",\n",
    "            \"tabs\",\n",
    "            \"unlimitedStorage\",\n",
    "            \"storage\",\n",
    "            \"<all_urls>\",\n",
    "            \"webRequest\",\n",
    "            \"webRequestBlocking\"\n",
    "        ],\n",
    "        \"background\": {\n",
    "            \"scripts\": [\"background.js\"]\n",
    "        },\n",
    "        \"minimum_chrome_version\":\"22.0.0\"\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    background_js = \"\"\"\n",
    "    var config = {\n",
    "            mode: \"fixed_servers\",\n",
    "            rules: {\n",
    "            singleProxy: {\n",
    "                scheme: \"http\",\n",
    "                host: \"%s\",\n",
    "                port: parseInt(%s)\n",
    "            },\n",
    "            bypassList: [\"localhost\"]\n",
    "            }\n",
    "        };\n",
    " \n",
    "    chrome.proxy.settings.set({value: config, scope: \"regular\"}, function() {});\n",
    " \n",
    "    function callbackFn(details) {\n",
    "        return {\n",
    "            authCredentials: {\n",
    "                username: \"%s\",\n",
    "                password: \"%s\"\n",
    "            }\n",
    "        };\n",
    "    }\n",
    " \n",
    "    chrome.webRequest.onAuthRequired.addListener(\n",
    "                callbackFn,\n",
    "                {urls: [\"<all_urls>\"]},\n",
    "                ['blocking']\n",
    "    );\n",
    "    \"\"\" % (proxy_host, proxy_port, proxy_user, proxy_pass)\n",
    "    pluginfile = 'proxy_auth_plugin.zip'\n",
    "\n",
    "    with zipfile.ZipFile(pluginfile, 'w') as zp:\n",
    "        zp.writestr(\"manifest.json\", manifest_json)\n",
    "        zp.writestr(\"background.js\", background_js)\n",
    "\n",
    "    return pluginfile   \n",
    "        \n",
    "proxyArgsList = [\n",
    "        {\n",
    "            'proxy_host': 'au.proxymesh.com',\n",
    "            'proxy_port': '31280',\n",
    "            'proxy_user': 'waleeProxy',\n",
    "            'proxy_pass': 'Cheeta@1',\n",
    "        },\n",
    "    ]\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Linux; Android 9; SM-N975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.101 Mobile Safari/537.36\"\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--window-size=150,500\")\n",
    "options.add_argument(\"--disable-automation\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument('--profile-directory=Default')\n",
    "options.add_argument(\"start-maximized\")\n",
    "options.add_argument(\"disable-infobars\")\n",
    "options.add_argument(\"--lang=en\")\n",
    "options.add_argument(\"--enable-javascript\")\n",
    "options.add_argument(\"--enable-cookies\")\n",
    "options.add_experimental_option(\"detach\", True)\n",
    "options.add_extension(getPlugin(**random.choice(proxyArgsList)))\n",
    "options.add_argument(f'--user-agent={user_agent}')\n",
    "\n",
    "googleSerp = []\n",
    "googleSerpIds = []\n",
    "\n",
    "try:\n",
    "    with open('googleSerp_v2.json', 'r', encoding='utf-8') as f:\n",
    "        googleSerp = json.load(f)\n",
    "        if googleSerp:\n",
    "            for item in googleSerp:\n",
    "                for key, value in item.items():\n",
    "                    googleSerpIds.append(key)\n",
    "\n",
    "    print(len(googleSerpIds))\n",
    "except:\n",
    "    print(\"NO PREVIOUS RECORD FOUND!\")\n",
    "    pass\n",
    "\n",
    "# requests_html used for company, service:\n",
    "\n",
    "# async def get_company_info(url):\n",
    "#     proxies = {\n",
    "#         'http': 'http://waleeProxy:Cheeta@1@au.proxymesh.com:31280',\n",
    "#         'https': 'http://waleeProxy:Cheeta@1@au.proxymesh.com:31280',\n",
    "#     }\n",
    "#     ua = UserAgent(browsers=['edge', 'chrome'])\n",
    "#     headers = {\n",
    "#         \"User-Agent\": f\"{ua.random}\"\n",
    "#     }\n",
    "#     session = AsyncHTMLSession()\n",
    "#     response = await session.get(url, headers=headers, proxies=proxies)\n",
    "#     html_content = response.html.html\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#     main_div = soup.find('div', {'role': 'main'})\n",
    "#     if main_div:\n",
    "#         company_url = \"\"\n",
    "#         service_var = \"\"\n",
    "        \n",
    "#         try:\n",
    "#             company_url_element = main_div.find('a', {'data-tooltip': 'Open website'})\n",
    "#             if company_url_element:\n",
    "#                 company_url_temp = company_url_element.get('href') if company_url_element else \"\"\n",
    "#                 company_url = await company_url_temp.split(\"q=\")[1].split(\"&opi\")[0] if company_url_temp else \"\"\n",
    "#         except:\n",
    "#             print(\"COMPANY_URL NOT FOUND!\")\n",
    "#             pass\n",
    "        \n",
    "#         try:  \n",
    "#             service_element = main_div.find('div', {'class': 'lMbq3e'}).find('div', {'class': 'fontBodyMedium'})\n",
    "#             if service_element:\n",
    "#                 following_sibling = service_element.find_next_sibling('div')\n",
    "#                 service_var = await following_sibling.text if following_sibling else \"\"\n",
    "#         except:\n",
    "#             print(\"SERVICE_VAR NOT FOUND!\")\n",
    "#             pass\n",
    "        \n",
    "#         await session.close()\n",
    "#         return await company_url, service_var\n",
    "    \n",
    "#     await session.close()\n",
    "#     return \"\", \"\"\n",
    "\n",
    "def get_company_info(driver):\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@role=\"main\"]')))\n",
    "    main_div = driver.find_element(By.XPATH, '//div[@role=\"main\"]')\n",
    "    if main_div:\n",
    "        company = \"\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@role=\"main\"]//a[contains(@data-tooltip, \"Open website\")]')))\n",
    "            company_element = main_div.find_element(By.XPATH, '//a[contains(@data-tooltip, \"Open website\")]')\n",
    "            if company_element:\n",
    "                company = company_element.get_attribute('href')\n",
    "                print(\"COMPANY_URL: \", company)\n",
    "        except:\n",
    "            print(\"COMPANY_URL NOT FOUND!\")\n",
    "            pass   \n",
    "        \n",
    "        service = \"\"\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@role=\"main\"]//div[contains(@class, \"lMbq3e\")]//div[@class=\"fontBodyMedium\"]')))\n",
    "            service_element = main_div.find_element(By.XPATH, '//div[contains(@class, \"lMbq3e\")]//div[@class=\"fontBodyMedium\"]')\n",
    "            if service_element:\n",
    "                service = service_element.text\n",
    "                print(\"SERVICE_VAR: \", service)\n",
    "        except:\n",
    "            print(\"SERVICE_VAR NOT FOUND!\")\n",
    "            pass   \n",
    "        return company, service\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "printCounter = 0\n",
    "with webdriver.Chrome(options=options) as driver:\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    counterPlace = 0\n",
    "    for place in placeDetails:\n",
    "        service_var = \"\"\n",
    "        company_url = \"\"\n",
    "        try:\n",
    "            print(\"\\n\", printCounter, \":\")\n",
    "            printCounter += 1\n",
    "            if place['_id'] not in googleSerpIds:\n",
    "                url = place['result']['url']\n",
    "                if url != '':\n",
    "                    driver.get(url)                            \n",
    "                    time.sleep(7)\n",
    "                    \n",
    "                    company_url, service_var = get_company_info(driver)\n",
    "                    \n",
    "                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"lMbq3e\"]//span[contains(@aria-label, \"review\")]')))\n",
    "                    reviews_click = driver.find_element(By.XPATH, '//div[@class=\"lMbq3e\"]//span[contains(@aria-label, \"review\")]')\n",
    "                    reviews_click.click()       \n",
    "                    time.sleep(3)\n",
    "                    \n",
    "                    counter = 0\n",
    "                    break_bool = False\n",
    "                    scroll_counter = 0\n",
    "                    prev_scroll_height = 0\n",
    "                    while True:\n",
    "                        all_reviews = driver.find_elements(By.XPATH, '//div[@class=\"hjmQqc\"]//div[@class=\"QGH3wd Inlyae\"]')\n",
    "                        if len(all_reviews) > counter:\n",
    "                            counter += 1\n",
    "                            \n",
    "                        total_scroll_counter = 0 \n",
    "                        for review in all_reviews[counter-1:counter]:\n",
    "                            if total_scroll_counter <= 120:\n",
    "                                total_scroll_counter += 1\n",
    "                                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, '//div[@class=\"hjmQqc\"]//div[@class=\"QGH3wd Inlyae\"]')))\n",
    "                                driver.execute_script(\"arguments[0].scrollIntoView(true);\", review)        \n",
    "                                time.sleep(0.3)\n",
    "                                current_scroll_height = driver.execute_script(\"return Math.max( document.evaluate(\\\"//div[@class='nkePVe']\\\", document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null).singleNodeValue.scrollHeight );\")\n",
    "                                if current_scroll_height == prev_scroll_height:\n",
    "                                    scroll_counter += 1\n",
    "                                else:\n",
    "                                    scroll_counter = 0\n",
    "                                    \n",
    "                                if scroll_counter > 9 and current_scroll_height == prev_scroll_height:\n",
    "                                    break_bool = True\n",
    "                                    break\n",
    "\n",
    "                                prev_scroll_height = current_scroll_height\n",
    "                            else:\n",
    "                                break_bool = True\n",
    "                                break\n",
    "                        if break_bool:\n",
    "                            break\n",
    "                    \n",
    "                    visible_reviews = driver.find_elements(By.XPATH, '//div[@class=\"hjmQqc\"]')\n",
    "                    reviews = []\n",
    "                    if visible_reviews:\n",
    "                        for index, review in enumerate(visible_reviews):\n",
    "                            item = {}\n",
    "                            html_content = review.get_attribute('outerHTML')\n",
    "                            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "                            item['Counter'] = index + 1\n",
    "                            name_element = soup.find('div', class_='IaK8zc CVo7Bb')\n",
    "                            name = name_element.text\n",
    "                            item['Customer'] = name\n",
    "                            rating_element = soup.find('div', class_='nSAWhf').find('div')\n",
    "                            rating = rating_element['aria-label'].split(\"Rating of \")[1]\n",
    "                            item['Stars'] = rating\n",
    "                            time_element = soup.find('div', class_='bHyEBc')\n",
    "                            time_t = time_element.text\n",
    "                            item['Time'] = time_t\n",
    "                            description_element = soup.find('div', class_='nSAWhf').find('span')\n",
    "                            description = description_element.text\n",
    "                            item['Review'] = description\n",
    "                            reviews.append(item)\n",
    "                            if index >= 49:\n",
    "                                break\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    placeDetails[counterPlace]['result']['reviews'] = reviews\n",
    "                    print(placeDetails[counterPlace]['result']['name'])\n",
    "                    \n",
    "                    item = {}\n",
    "                    item[place['_id']] = {\n",
    "                        'company_info': True,\n",
    "                        'company_name': place['result']['name'] if 'name' in place['result'] else \"\",\n",
    "                        'address': place['result']['formatted_address'] if 'formatted_address' in place['result'] else \"\",\n",
    "                        'phone_number': place['result']['formatted_phone_number'] if 'formatted_phone_number' in place['result'] else \"\",\n",
    "                        'international_phone_number': place['result']['international_phone_number'] if 'international_phone_number' in place['result'] else \"\",\n",
    "                        'avg_customer_reviews': place['result']['rating'] if 'rating' in place['result'] else \"\",\n",
    "                        'no_of_customer_reviews': place['result']['user_ratings_total'] if 'user_ratings_total' in place['result'] else \"\",\n",
    "                        'service': service_var,\n",
    "                        'reviews': place['result']['reviews'] if 'reviews' in place['result'] else [],\n",
    "                        'google_maps_url': place['result']['url'] if 'url' in place['result'] else \"\",\n",
    "                        'company_url': company_url\n",
    "                    }\n",
    "                    with open('tempGoogleSerp_v2.json', 'a', encoding='utf-8') as f:\n",
    "                        json.dump(item, f, ensure_ascii=False, indent=2)\n",
    "                        f.write(',\\n')\n",
    "                    counterPlace += 1                          \n",
    "                else:\n",
    "                    counterPlace += 1\n",
    "                    continue\n",
    "            else:\n",
    "                print(\"ALREADY_CRAWLED: \", place['result']['name'])\n",
    "                counterPlace += 1\n",
    "                continue\n",
    "        except:\n",
    "            try:   \n",
    "                placeDetails[counterPlace]['result']['reviews'] = []\n",
    "                print(placeDetails[counterPlace]['result']['name'])\n",
    "                \n",
    "                item = {}\n",
    "                item[place['_id']] = {\n",
    "                    'company_info': True,\n",
    "                    'company_name': place['result']['name'] if 'name' in place['result'] else \"\",\n",
    "                    'address': place['result']['formatted_address'] if 'formatted_address' in place['result'] else \"\",\n",
    "                    'phone_number': place['result']['formatted_phone_number'] if 'formatted_phone_number' in place['result'] else \"\",\n",
    "                    'international_phone_number': place['result']['international_phone_number'] if 'international_phone_number' in place['result'] else \"\",\n",
    "                    'avg_customer_reviews': place['result']['rating'] if 'rating' in place['result'] else \"\",\n",
    "                    'no_of_customer_reviews': place['result']['user_ratings_total'] if 'user_ratings_total' in place['result'] else \"\",\n",
    "                    'service': service_var,\n",
    "                    'reviews': place['result']['reviews'] if 'reviews' in place['result'] else [],\n",
    "                    'google_maps_url': place['result']['url'] if 'url' in place['result'] else \"\",\n",
    "                    'company_url': company_url\n",
    "                }\n",
    "                \n",
    "                with open('tempGoogleSerp_v2.json', 'a', encoding='utf-8') as f:\n",
    "                    json.dump(item, f, ensure_ascii=False, indent=2)\n",
    "                    f.write(',\\n')\n",
    "\n",
    "                counterPlace += 1\n",
    "                continue\n",
    "            except:\n",
    "                print(\"ERROR PARSING: \", place['result']['name'])\n",
    "                pass\n",
    "            counterPlace += 1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# data = []\n",
    "# for index, place in enumerate(placeDetails):\n",
    "#     if place['result']['url']:\n",
    "#         item = {}\n",
    "#         item[names[index].title()] = {\n",
    "#             'company_info': True,\n",
    "#             'company_name': place['result']['name'],\n",
    "#             'address': place['result']['formatted_address'],\n",
    "#             'phone_number': place['result']['formatted_phone_number'],\n",
    "#             'international_phone_number': place['result']['international_phone_number'],\n",
    "#             'avg. customer reviews': place['result']['rating'],\n",
    "#             'no of. customer reviews': place['result']['user_ratings_total'],\n",
    "#             'reviews': place['result']['reviews'],\n",
    "#             'url': place['result']['url']\n",
    "#         }\n",
    "#         data.append(item)\n",
    "#     else:\n",
    "#         item = {}\n",
    "#         item[names[index].title()] = {\n",
    "#             'company_info': False,\n",
    "#             'company_name': \"\",\n",
    "#             'address': \"\",\n",
    "#             'phone_number': \"\",\n",
    "#             'international_phone_number': \"\",\n",
    "#             'avg. customer reviews': \"\",\n",
    "#             'no of. customer reviews': \"\",\n",
    "#             'reviews': [],\n",
    "#             'url': \"\"\n",
    "#         }\n",
    "#         data.append(item)\n",
    "    \n",
    "# with open('googleSerp_v1.json', 'a', encoding='utf-8') as f:\n",
    "#     json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
